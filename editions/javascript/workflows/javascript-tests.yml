name: JavaScript/TypeScript Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - '**/*.js'
      - '**/*.jsx'
      - '**/*.ts'
      - '**/*.tsx'
      - '**/*.test.*'
      - '**/*.spec.*'
      - 'package.json'
      - 'package-lock.json'
      - 'jest.config.*'
      - 'vitest.config.*'
      - 'playwright.config.*'
  pull_request:
    branches: [ main, develop ]
    paths:
      - '**/*.js'
      - '**/*.jsx'
      - '**/*.ts'
      - '**/*.tsx'
      - '**/*.test.*'
      - '**/*.spec.*'
      - 'package.json'
      - 'package-lock.json'
      - 'jest.config.*'
      - 'vitest.config.*'
      - 'playwright.config.*'

concurrency:
  group: js-tests-${{ github.ref }}
  cancel-in-progress: true

env:
  # Test environment variables
  NODE_ENV: test
  CI: true

jobs:
  # Unit and Integration Tests
  unit-tests:
    name: Unit & Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    strategy:
      matrix:
        node-version: [18, 20]
        test-type: ['unit', 'integration']
      fail-fast: false

    steps:
      - name: ğŸ”„ Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ğŸŸ¢ Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: |
            package-lock.json
            */package-lock.json

      - name: ğŸ“¦ Cache npm dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
            */node_modules
          key: ${{ runner.os }}-npm-test-${{ matrix.node-version }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-test-${{ matrix.node-version }}-
            ${{ runner.os }}-npm-test-
            ${{ runner.os }}-npm-

      - name: ğŸ§ª Cache Jest
        uses: actions/cache@v4
        with:
          path: |
            .jestcache
            coverage
          key: ${{ runner.os }}-jest-${{ matrix.node-version }}-${{ hashFiles('**/jest.config.*', '**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-jest-${{ matrix.node-version }}-
            ${{ runner.os }}-jest-

      - name: ğŸ”§ Install Dependencies
        run: |
          npm ci --prefer-offline --no-audit
          # Install test-specific global tools
          if command -v jest &> /dev/null; then
            echo "Jest already available"
          else
            npm install -g jest
          fi

      - name: ğŸ§ª Run Unit Tests
        if: matrix.test-type == 'unit'
        run: |
          echo "ğŸ§ª Running unit tests..."
          if npm run test:unit --if-present; then
            echo "âœ… Unit tests passed"
          elif npm run test -- --testPathPattern=".*\.(test|spec)\.(js|ts|tsx)$" --testPathIgnorePatterns="e2e|integration" --if-present; then
            echo "âœ… Unit tests passed (fallback pattern)"
          elif npm test --if-present; then
            echo "âœ… Tests passed (default command)"
          else
            echo "âš ï¸ No unit test command found"
          fi

      - name: ğŸ”— Run Integration Tests
        if: matrix.test-type == 'integration'
        run: |
          echo "ğŸ”— Running integration tests..."
          if npm run test:integration --if-present; then
            echo "âœ… Integration tests passed"
          elif npm run test -- --testPathPattern=".*integration.*\.(test|spec)\.(js|ts|tsx)$" --if-present; then
            echo "âœ… Integration tests passed (pattern match)"
          else
            echo "âš ï¸ No integration test command found - skipping"
            exit 0
          fi

      - name: ğŸ“Š Generate Coverage Report
        if: matrix.test-type == 'unit'
        run: |
          echo "ğŸ“Š Generating coverage report..."
          if npm run test:coverage --if-present; then
            echo "âœ… Coverage report generated"
          elif npm run test -- --coverage --if-present; then
            echo "âœ… Coverage report generated (with flag)"
          else
            echo "âš ï¸ No coverage command available"
          fi

      - name: ğŸ“¤ Upload Coverage to Codecov
        if: matrix.test-type == 'unit' && hashFiles('coverage/lcov.info') != ''
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage/lcov.info
          fail_ci_if_error: false
          verbose: true
          flags: unit-tests
          name: coverage-${{ matrix.node-version }}

      - name: ğŸ“¤ Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-type }}-node-${{ matrix.node-version }}
          path: |
            coverage/
            test-results/
            reports/
            .jestcache/
          retention-days: 5

  # E2E Tests (separate job for better resource management)
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'

    steps:
      - name: ğŸ”„ Checkout Repository
        uses: actions/checkout@v4

      - name: ğŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: ğŸ“¦ Cache npm dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ runner.os }}-npm-e2e-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-npm-e2e-
            ${{ runner.os }}-npm-

      - name: ğŸ­ Cache Playwright Browsers
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: ğŸ”§ Install Dependencies
        run: |
          npm ci --prefer-offline --no-audit

      - name: ğŸ­ Install Playwright Browsers
        run: |
          if command -v playwright &> /dev/null || npm list playwright &> /dev/null; then
            echo "ğŸ­ Installing Playwright browsers..."
            npx playwright install --with-deps
          elif command -v cypress &> /dev/null || npm list cypress &> /dev/null; then
            echo "ğŸŒ² Cypress detected - installing dependencies..."
            npx cypress install
          else
            echo "âš ï¸ No E2E framework detected - skipping browser installation"
          fi

      - name: ğŸš€ Build Application
        run: |
          echo "ğŸš€ Building application for E2E tests..."
          if npm run build --if-present; then
            echo "âœ… Build successful"
          else
            echo "âš ï¸ No build script found"
          fi

      - name: ğŸ­ Run Playwright Tests
        if: hashFiles('**/playwright.config.*') != ''
        run: |
          echo "ğŸ­ Running Playwright E2E tests..."
          npm run test:e2e --if-present || npx playwright test

      - name: ğŸŒ² Run Cypress Tests
        if: hashFiles('**/cypress.config.*') != '' && hashFiles('**/playwright.config.*') == ''
        run: |
          echo "ğŸŒ² Running Cypress E2E tests..."
          npm run cypress:run --if-present || npx cypress run

      - name: ğŸ“¤ Upload E2E Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: |
            test-results/
            playwright-report/
            cypress/screenshots/
            cypress/videos/
          retention-days: 7

  # Test Summary and Quality Gates
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, e2e-tests]
    if: always()

    steps:
      - name: ğŸ“Š Download Test Artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./test-artifacts

      - name: ğŸ“ˆ Generate Test Summary
        run: |
          echo "ğŸ“ˆ Generating comprehensive test summary..."
          
          # Create summary report
          mkdir -p reports
          cat > reports/test-summary.md << 'EOF'
          # ğŸ§ª Test Execution Summary
          
          ## Overview
          - **Workflow**: ${{ github.workflow }}
          - **Trigger**: ${{ github.event_name }}
          - **Branch**: ${{ github.ref_name }}
          - **Commit**: ${{ github.sha }}
          - **Timestamp**: $(date)
          
          ## Results
          EOF
          
          # Check unit test results
          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "âœ… **Unit Tests**: PASSED" >> reports/test-summary.md
          else
            echo "âŒ **Unit Tests**: FAILED" >> reports/test-summary.md
          fi
          
          # Check E2E test results
          if [ "${{ needs.e2e-tests.result }}" == "success" ]; then
            echo "âœ… **E2E Tests**: PASSED" >> reports/test-summary.md
          elif [ "${{ needs.e2e-tests.result }}" == "skipped" ]; then
            echo "â­ï¸ **E2E Tests**: SKIPPED" >> reports/test-summary.md
          else
            echo "âŒ **E2E Tests**: FAILED" >> reports/test-summary.md
          fi
          
          # Overall status
          echo "" >> reports/test-summary.md
          if [ "${{ needs.unit-tests.result }}" == "success" ] && ([ "${{ needs.e2e-tests.result }}" == "success" ] || [ "${{ needs.e2e-tests.result }}" == "skipped" ]); then
            echo "ğŸ‰ **Overall Status**: ALL TESTS PASSED" >> reports/test-summary.md
            echo "test_status=success" >> $GITHUB_ENV
          else
            echo "ğŸ’¥ **Overall Status**: TESTS FAILED" >> reports/test-summary.md
            echo "test_status=failure" >> $GITHUB_ENV
          fi
          
          # Display summary
          cat reports/test-summary.md

      - name: ğŸ“¤ Upload Final Test Summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary-report
          path: reports/test-summary.md
          retention-days: 30

      - name: âœ… Mark Success
        if: env.test_status == 'success'
        run: |
          echo "ğŸ‰ All tests passed successfully!"

      - name: âŒ Mark Failure
        if: env.test_status == 'failure'
        run: |
          echo "ğŸ’¥ Some tests failed!"
          exit 1

  # Quality Gate Check
  quality-gate:
    name: Quality Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, e2e-tests]
    if: always() && github.event_name == 'pull_request'

    steps:
      - name: ğŸšª Quality Gate Assessment
        run: |
          echo "ğŸšª Assessing quality gate criteria..."
          
          # Define quality criteria
          unit_passed=false
          e2e_passed_or_skipped=false
          
          # Check unit tests (mandatory)
          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            unit_passed=true
            echo "âœ… Unit tests: PASSED"
          else
            echo "âŒ Unit tests: FAILED (blocking)"
          fi
          
          # Check E2E tests (can be skipped)
          if [ "${{ needs.e2e-tests.result }}" == "success" ] || [ "${{ needs.e2e-tests.result }}" == "skipped" ]; then
            e2e_passed_or_skipped=true
            echo "âœ… E2E tests: OK"
          else
            echo "âŒ E2E tests: FAILED (blocking)"
          fi
          
          # Final quality gate decision
          if [ "$unit_passed" == "true" ] && [ "$e2e_passed_or_skipped" == "true" ]; then
            echo "ğŸ‰ Quality gate: PASSED - Ready to merge!"
            exit 0
          else
            echo "ğŸš« Quality gate: FAILED - Cannot merge"
            exit 1
          fi 